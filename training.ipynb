{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-08T07:43:28.236859Z",
     "iopub.status.busy": "2025-08-08T07:43:28.236624Z",
     "iopub.status.idle": "2025-08-08T07:43:45.983107Z",
     "shell.execute_reply": "2025-08-08T07:43:45.982334Z",
     "shell.execute_reply.started": "2025-08-08T07:43:28.236836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset downloads...\n",
      "‚¨áÔ∏è Downloading Cornell dataset...\n",
      "‚úÖ Downloaded: 9.46 MB\n",
      "üìÇ Extracting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading IMSDb: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:03<00:00,  4.82it/s]\n",
      "Downloading Springfield: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:12<00:00,  1.41it/s]\n",
      "Downloading ScreenplayDB:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è 12-angry-men: HTTPSConnectionPool(host='www.screenplaydb.com', port=443): Max retries exceeded with url: /film/scripts/12-angry-men.txt (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1016)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ScreenplayDB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è birdman: HTTPSConnectionPool(host='www.screenplaydb.com', port=443): Max retries exceeded with url: /film/scripts/birdman.txt (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1016)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 1. SETUP ENVIRONMENT\n",
    "# ======================\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Setup directories\n",
    "!rm -rf /kaggle/working/*\n",
    "!mkdir -p /kaggle/working/{data,cleaned_scripts,logs}\n",
    "           \n",
    "!mkdir -p /kaggle/working/data/{raw,preprocessed}/{cornell,imsdb,springfield,screenplaydb}\n",
    "\n",
    "# ======================\n",
    "# 2. DOWNLOAD ALL DATASETS\n",
    "# ======================\n",
    "def download_cornell():\n",
    "    cornell_url = \"https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "    cornell_zip = \"/kaggle/working/data/raw/cornell.zip\"\n",
    "\n",
    "    print(\"‚¨áÔ∏è Downloading Cornell dataset...\")\n",
    "    response = requests.get(cornell_url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(cornell_zip, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Verify download\n",
    "    if os.path.exists(cornell_zip):\n",
    "        print(f\"‚úÖ Downloaded: {os.path.getsize(cornell_zip)/1024/1024:.2f} MB\")\n",
    "    else:\n",
    "        raise Exception(\"Download failed!\")\n",
    "\n",
    "    # Extract\n",
    "    print(\"üìÇ Extracting...\")\n",
    "    with zipfile.ZipFile(cornell_zip, 'r') as z:\n",
    "        z.extractall(\"/kaggle/working/data/raw/cornell\")\n",
    "\n",
    "def download_imsdb_script(movie):\n",
    "    try:\n",
    "        url = f\"https://imsdb.com/scripts/{movie}.html\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            text = re.search(r'<pre>(.*?)</pre>', response.text, re.DOTALL)\n",
    "            if text:\n",
    "                clean_text = re.sub(r'<.*?>', '', text.group(1))\n",
    "                with open(f\"/kaggle/working/data/raw/imsdb/{movie}.txt\", 'w') as f:\n",
    "                    f.write(clean_text)\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {movie}: {str(e)}\")\n",
    "    return False\n",
    "\n",
    "def download_springfield_script(movie):\n",
    "    try:\n",
    "        url = f\"https://www.springfieldspringfield.co.uk/movie_script.php?movie={movie}\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            text = re.search(r'<div class=\"scrolling-script-container\">(.*?)</div>', response.text, re.DOTALL)\n",
    "            if text:\n",
    "                clean_text = re.sub(r'<.*?>', '', text.group(1))\n",
    "                clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "                with open(f\"/kaggle/working/data/raw/springfield/{movie}.txt\", 'w') as f:\n",
    "                    f.write(clean_text)\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {movie}: {str(e)}\")\n",
    "    return False\n",
    "\n",
    "def download_screenplaydb_script(movie, url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(f\"/kaggle/working/data/raw/screenplaydb/{movie}.txt\", 'w') as f:\n",
    "                f.write(response.text)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {movie}: {str(e)}\")\n",
    "    return False\n",
    "\n",
    "# Download all datasets\n",
    "print(\"Starting dataset downloads...\")\n",
    "download_cornell()\n",
    "\n",
    "# IMSDb scripts\n",
    "imsdb_movies = [\n",
    "    'Pulp-Fiction', 'The-Matrix', 'Inception',\n",
    "    'The-Dark-Knight', 'Fight-Club', 'Forrest-Gump',\n",
    "    'Good-Will-Hunting', 'The-Shawshank-Redemption',\n",
    "    'The-Godfather', 'The-Social-Network',\n",
    "    'Interstellar', 'The-Prestige', 'Se7en',\n",
    "    'The-Truman-Show', 'American-Beauty']\n",
    "for movie in tqdm(imsdb_movies, desc=\"Downloading IMSDb\"):\n",
    "    download_imsdb_script(movie)\n",
    "\n",
    "# Springfield scripts\n",
    "springfield_movies = [\n",
    "    'alien', 'blade-runner', 'casablanca',\n",
    "    'citizen-kane', 'gravity', 'her',\n",
    "    'jurassic-park', 'la-la-land', 'mad-max-fury-road',\n",
    "    'the-martian', 'psycho', 'the-shining',\n",
    "    'eternal-sunshine-of-the-spotless-mind', 'the-grand-budapest-hotel',\n",
    "    'moonlight', 'parasite', 'whiplash'\n",
    "]\n",
    "\n",
    "for movie in tqdm(springfield_movies, desc=\"Downloading Springfield\"):\n",
    "    download_springfield_script(movie)\n",
    "\n",
    "\n",
    "# ScreenplayDB scripts\n",
    "screenplaydb_movies = {\n",
    "    '12-angry-men': 'https://www.screenplaydb.com/film/scripts/12-angry-men.txt',\n",
    "    'birdman': 'https://www.screenplaydb.com/film/scripts/birdman.txt'\n",
    "}\n",
    "for movie, url in tqdm(screenplaydb_movies.items(), desc=\"Downloading ScreenplayDB\"):\n",
    "    download_screenplaydb_script(movie, url)\n",
    "\n",
    "# ======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T07:43:45.987426Z",
     "iopub.status.busy": "2025-08-08T07:43:45.987143Z",
     "iopub.status.idle": "2025-08-08T07:43:45.994569Z",
     "shell.execute_reply": "2025-08-08T07:43:45.993896Z",
     "shell.execute_reply.started": "2025-08-08T07:43:45.987391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DOWNLOAD VERIFICATION:\n",
      "\n",
      "üìÇ Cornell:\n",
      "7 files\n",
      "\n",
      "üìÇ IMSDb:\n",
      "15 scripts\n",
      "\n",
      "üìÇ Springfield:\n",
      "17 scripts\n",
      "\n",
      "üìÇ ScreenplayDB:\n",
      "0 scripts\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 3. VERIFY DOWNLOADS\n",
    "# ======================\n",
    "def count_files(directory, extension=\"*.txt\"):\n",
    "    return len(list(Path(directory).rglob(extension)))\n",
    "\n",
    "def verify_downloads():\n",
    "    print(\"\\nüîç DOWNLOAD VERIFICATION:\")\n",
    "    \n",
    "    print(\"\\nüìÇ Cornell:\")\n",
    "    cornell_files = count_files(\"/kaggle/working/data/raw/cornell\")\n",
    "    print(f\"{cornell_files} files\")\n",
    "    \n",
    "    print(\"\\nüìÇ IMSDb:\")\n",
    "    imsdb_files = count_files(\"/kaggle/working/data/raw/imsdb\")\n",
    "    print(f\"{imsdb_files} scripts\")\n",
    "    \n",
    "    print(\"\\nüìÇ Springfield:\")\n",
    "    springfield_files = count_files(\"/kaggle/working/data/raw/springfield\")\n",
    "    print(f\"{springfield_files} scripts\")\n",
    "    \n",
    "    print(\"\\nüìÇ ScreenplayDB:\")\n",
    "    screenplaydb_files = count_files(\"/kaggle/working/data/raw/screenplaydb\")\n",
    "    print(f\"{screenplaydb_files} scripts\")\n",
    "\n",
    "verify_downloads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T07:43:45.995670Z",
     "iopub.status.busy": "2025-08-08T07:43:45.995458Z",
     "iopub.status.idle": "2025-08-08T07:43:46.618076Z",
     "shell.execute_reply": "2025-08-08T07:43:46.617358Z",
     "shell.execute_reply.started": "2025-08-08T07:43:45.995653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 304403 dialogues to /kaggle/working/cleaned_scripts/cornell_dialogues.txt\n",
      "\n",
      "üîç First 5 Cornell dialogues:\n",
      "BIANCA: They do not!\n",
      "CAMERON: They do to!\n",
      "BIANCA: I hope so.\n",
      "CAMERON: She okay?\n",
      "BIANCA: Let's go.\n",
      "\n",
      "üìú Total lines: 304403\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 4. PROCESS CORNELL DATA\n",
    "# ======================\n",
    "def clean_cornell(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        if line.count('+++$+++') < 4:\n",
    "            continue\n",
    "            \n",
    "        parts = line.split(' +++$+++ ')\n",
    "        if len(parts) < 5:\n",
    "            continue\n",
    "            \n",
    "        character = parts[3].strip().upper()\n",
    "        dialogue = parts[4].strip()\n",
    "        \n",
    "        if character and dialogue:\n",
    "            cleaned.append(f\"{character}: {dialogue}\")\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(cleaned))\n",
    "    print(f\"‚úÖ Saved {len(cleaned)} dialogues to {output_path}\")\n",
    "\n",
    "cornell_input = \"/kaggle/working/data/raw/cornell/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "cornell_output = \"/kaggle/working/cleaned_scripts/cornell_dialogues.txt\"\n",
    "clean_cornell(cornell_input, cornell_output)\n",
    "\n",
    "# Verify Cornell output\n",
    "print(\"\\nüîç First 5 Cornell dialogues:\")\n",
    "!head -n 5 {cornell_output}\n",
    "print(f\"\\nüìú Total lines: {sum(1 for _ in open(cornell_output))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T07:43:46.620034Z",
     "iopub.status.busy": "2025-08-08T07:43:46.619367Z",
     "iopub.status.idle": "2025-08-08T07:43:47.345059Z",
     "shell.execute_reply": "2025-08-08T07:43:47.344520Z",
     "shell.execute_reply.started": "2025-08-08T07:43:46.620002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing Cornell movie dialogues...\n",
      "‚úÖ Saved 304403 dialogues to /kaggle/working/data/preprocessed/cornell/cornell_dialogues.txt\n",
      "\n",
      "üîÑ Processing imsdb scripts...\n",
      "\n",
      "üîÑ Processing springfield scripts...\n",
      "\n",
      "üîÑ Processing screenplaydb scripts...\n",
      "\n",
      "üîç PREPROCESSING VERIFICATION:\n",
      "\n",
      "üìÇ Cornell (Preprocessed):\n",
      "1 files\n",
      "Sample line: BIANCA: They do not!\n",
      "\n",
      "üìÇ IMSDb (Preprocessed):\n",
      "15 scripts\n",
      "\n",
      "üìÇ Springfield (Preprocessed):\n",
      "17 scripts\n",
      "\n",
      "üìÇ ScreenplayDB (Preprocessed):\n",
      "0 scripts\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 5. PROCESS SCREENPLAYS\n",
    "# ======================\n",
    "def clean_screenplay(text):\n",
    "    text = re.sub(r'<.*?>|http\\S+|\\(.*?\\)|\\*.*?\\*|\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'^.*?(INT\\.|EXT\\.|FADE IN:)', r'\\1', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    text = re.sub(r'(CUT TO:|DISSOLVE TO:|FADE OUT\\.).*?\\n', '', text)\n",
    "    text = re.sub(r'^\\s*([A-Z][A-Z\\s]+)\\s*$', r'\\1:', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\':-]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def process_all_files():\n",
    "    \"\"\"Process all datasets including Cornell movie dialogues\"\"\"\n",
    "    # First process Cornell dataset (special handling)\n",
    "    print(\"\\nüîÑ Processing Cornell movie dialogues...\")\n",
    "    cornell_input = \"/kaggle/working/data/raw/cornell/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "    cornell_output = \"/kaggle/working/data/preprocessed/cornell/cornell_dialogues.txt\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(\"/kaggle/working/data/preprocessed/cornell\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    clean_cornell(cornell_input, cornell_output)\n",
    "\n",
    "    # Then process screenplay datasets\n",
    "    sources = {\n",
    "        \"imsdb\": \"/kaggle/working/data/raw/imsdb\",\n",
    "        \"springfield\": \"/kaggle/working/data/raw/springfield\", \n",
    "        \"screenplaydb\": \"/kaggle/working/data/raw/screenplaydb\"\n",
    "    }\n",
    "    \n",
    "    for source, path in sources.items():\n",
    "        print(f\"\\nüîÑ Processing {source} scripts...\")\n",
    "        Path(f\"/kaggle/working/data/preprocessed/{source}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for script in Path(path).glob(\"*.txt\"):\n",
    "            try:\n",
    "                with open(script, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                cleaned = clean_screenplay(content)\n",
    "                \n",
    "                output_path = f\"/kaggle/working/data/preprocessed/{source}/{script.stem}_clean.txt\"\n",
    "                \n",
    "                with open(output_path, 'w') as f_out:\n",
    "                    f_out.write(cleaned)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing {script.name}: {str(e)}\")\n",
    "\n",
    "# And the corresponding verify_preprocessing function:\n",
    "def verify_preprocessing():\n",
    "    print(\"\\nüîç PREPROCESSING VERIFICATION:\")\n",
    "    \n",
    "    # Verify Cornell processing\n",
    "    print(\"\\nüìÇ Cornell (Preprocessed):\")\n",
    "    cornell_files = list(Path(\"/kaggle/working/data/preprocessed/cornell\").glob(\"*.txt\"))\n",
    "    print(f\"{len(cornell_files)} files\")\n",
    "    if cornell_files:\n",
    "        with open(cornell_files[0], 'r') as f:\n",
    "            print(f\"Sample line: {f.readline().strip()}\")\n",
    "    \n",
    "    # Verify screenplays\n",
    "    print(\"\\nüìÇ IMSDb (Preprocessed):\")\n",
    "    imsdb_files = list(Path(\"/kaggle/working/data/preprocessed/imsdb\").glob(\"*_clean.txt\"))\n",
    "    print(f\"{len(imsdb_files)} scripts\")\n",
    "    \n",
    "    print(\"\\nüìÇ Springfield (Preprocessed):\")\n",
    "    springfield_files = list(Path(\"/kaggle/working/data/preprocessed/springfield\").glob(\"*_clean.txt\"))\n",
    "    print(f\"{len(springfield_files)} scripts\")\n",
    "    \n",
    "    print(\"\\nüìÇ ScreenplayDB (Preprocessed):\")\n",
    "    screenplaydb_files = list(Path(\"/kaggle/working/data/preprocessed/screenplaydb\").glob(\"*_clean.txt\"))\n",
    "    print(f\"{len(screenplaydb_files)} scripts\")\n",
    "\n",
    "# Run the processing\n",
    "process_all_files()\n",
    "verify_preprocessing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T07:44:15.234043Z",
     "iopub.status.busy": "2025-08-08T07:44:15.233598Z",
     "iopub.status.idle": "2025-08-08T07:44:18.286234Z",
     "shell.execute_reply": "2025-08-08T07:44:18.285620Z",
     "shell.execute_reply.started": "2025-08-08T07:44:15.234022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcfee0ad7534535a1fd0a2bfa80a7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4253138c70184ed1aba482faeac455ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069e24e189f4be293e8ecb216a135e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be8d5e0b71a48d594277cf573f7506a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e651dea75d7e459596613e35eb8fbf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d601479794e400d856f11d5295139b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Emotion classifier loaded successfully (DistilBERT patched)\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 3. EMOTION CLASSIFICATION SETUP (FIXED)\n",
    "# ======================\n",
    "# 3. EMOTION CLASSIFICATION SETUP (FIXED PROPERLY)\n",
    "# ======================\n",
    "try:\n",
    "    \n",
    "    # Load components separately with proper config\n",
    "    model_name = \"adcg1355/moodmatemodels\"\n",
    "    model_name = \"adcg1355/moodmatemodels\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, truncation_side='left')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Patch model.forward to remove token_type_ids\n",
    "    original_forward = model.forward\n",
    "    def patched_forward(*args, **kwargs):\n",
    "        kwargs.pop('token_type_ids', None)\n",
    "        return original_forward(*args, **kwargs)\n",
    "    model.forward = patched_forward\n",
    "\n",
    "    # Create pipeline\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "         model=model,\n",
    "         tokenizer=tokenizer,\n",
    "         framework=\"pt\",\n",
    "         device=0 if torch.cuda.is_available() else -1,\n",
    "         truncation=True,\n",
    "         max_length=512,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Load tokenizer with special settings\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"‚úÖ Emotion classifier loaded successfully (DistilBERT patched)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load emotion classifier: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T07:44:18.287136Z",
     "iopub.status.busy": "2025-08-08T07:44:18.286911Z",
     "iopub.status.idle": "2025-08-08T07:44:18.292503Z",
     "shell.execute_reply": "2025-08-08T07:44:18.291776Z",
     "shell.execute_reply.started": "2025-08-08T07:44:18.287118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. EMOTION TAGGING FUNCTION\n",
    "# ======================\n",
    "def get_emotion(dialogue):\n",
    "    \"\"\"Ultra-robust emotion classification\"\"\"\n",
    "    try:\n",
    "        # Clean input\n",
    "        clean_text = ''.join(char for char in dialogue[:500] if char.isprintable())\n",
    "        \n",
    "        # Optional sleep to avoid rate limits (if needed)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Run emotion classifier\n",
    "        result = emotion_classifier(\n",
    "            clean_text,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        \n",
    "        return result[0] if isinstance(result, list) else {'label': 'neutral', 'score': 0.0}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped line (error: {str(e)}): {dialogue[:50]}\")\n",
    "        return {'label': 'neutral', 'score': 0.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:27:48.083185Z",
     "iopub.status.busy": "2025-08-08T08:27:48.082356Z",
     "iopub.status.idle": "2025-08-08T08:39:59.170837Z",
     "shell.execute_reply": "2025-08-08T08:39:59.169885Z",
     "shell.execute_reply.started": "2025-08-08T08:27:48.083151Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Emotion classifier loaded successfully (DistilBERT patched)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26dac2608c24a509e387714569023bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finalizing:   0%|          | 0/304403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully saved 304403 dialogues to:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/kaggle/working/data/cleaned_cornell.txt' target='_blank'>/kaggle/working/data/cleaned_cornell.txt</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/data/cleaned_cornell.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç FINAL VERIFICATION\n",
      "Total dialogues processed: 304403\n",
      "Sample metadata:\n",
      "{'character': 'BIANCA', 'emotion': 'neutral', 'score': 0.7312625646591187, 'source': 'cornell'}\n",
      "total 79M\n",
      "-rw-r--r-- 1 root root  79M Aug  8 08:39 cleaned_cornell.txt\n",
      "drwxr-xr-x 6 root root 4.0K Aug  8 07:43 preprocessed\n",
      "drwxr-xr-x 6 root root 4.0K Aug  8 07:43 raw\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# ======================\n",
    "# 1. PATH CONFIGURATION & SETUP\n",
    "# ======================\n",
    "# Define all paths\n",
    "CLEANED_INPUT = \"/kaggle/working/cleaned_scripts/cornell_dialogues.txt\"\n",
    "ENHANCED_OUTPUT = \"/kaggle/working/data/cleaned_cornell.txt\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path(CLEANED_INPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(ENHANCED_OUTPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# 2. SETUP THE EMOTION CLASSIFICATION PIPELINE (USING YOUR MODEL)\n",
    "# ======================\n",
    "try:\n",
    "    # Load components separately with proper config\n",
    "    model_name = \"adcg1355/moodmatemodels\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, truncation_side='left')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Patch model.forward to remove token_type_ids\n",
    "    original_forward = model.forward\n",
    "    def patched_forward(*args, **kwargs):\n",
    "        kwargs.pop('token_type_ids', None)\n",
    "        return original_forward(*args, **kwargs)\n",
    "    model.forward = patched_forward\n",
    "\n",
    "    # Create pipeline\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework=\"pt\",\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Emotion classifier loaded successfully (DistilBERT patched)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load emotion classifier: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ======================\n",
    "# 3. ENHANCED PROCESSING (MODIFIED)\n",
    "# ======================\n",
    "def enhance_with_emotion(input_path, output_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Processes dialogues with emotion tags using a batched approach for speed.\n",
    "    This function reads the entire file into a Dataset and then feeds it\n",
    "    to the Hugging Face pipeline in batches.\n",
    "    \"\"\"\n",
    "    # Create the correct mapping from numerical labels to emotion names from the GoEmotions dataset\n",
    "    # This list of 28 emotions is based on the default `id2label` mapping.\n",
    "    emotion_labels = [\n",
    "        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "        'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "        'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "        'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "        'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "    ]\n",
    "    emotion_map = {f'LABEL_{i}': label for i, label in enumerate(emotion_labels)}\n",
    "\n",
    "    # Verify input exists\n",
    "    if not Path(input_path).exists():\n",
    "        print(f\"‚ùå Input file not found: {input_path}\")\n",
    "        return []\n",
    "\n",
    "    enhanced = []\n",
    "    dialogues_to_process = []\n",
    "    \n",
    "    # Read all dialogue lines from the input file first\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if \": \" in line:\n",
    "                dialogues_to_process.append(line.strip())\n",
    "\n",
    "    if not dialogues_to_process:\n",
    "        print(\"‚ùå No dialogues found in the input file to process.\")\n",
    "        return []\n",
    "\n",
    "    # Create a Hugging Face Dataset from our list of dialogues\n",
    "    dataset = Dataset.from_dict({'text': dialogues_to_process})\n",
    "\n",
    "    try:\n",
    "        # Pass the dataset and batch size to the pipeline.\n",
    "        all_results = emotion_classifier(dataset['text'], batch_size=batch_size)\n",
    "\n",
    "        # Now, combine the original dialogues with the new emotion data\n",
    "        for dialogue, result in tqdm(zip(dialogues_to_process, all_results), total=len(dialogues_to_process), desc=\"Finalizing\"):\n",
    "            try:\n",
    "                character, text = dialogue.split(\": \", 1)\n",
    "                # Use the mapping to get the real emotion name\n",
    "                emotion_label = result['label']\n",
    "                emotion = emotion_map.get(emotion_label, \"unknown\") # Use .get to avoid errors\n",
    "                score = result['score']\n",
    "\n",
    "                enhanced.append({\n",
    "                    \"text\": dialogue,\n",
    "                    \"metadata\": {\n",
    "                        \"character\": character,\n",
    "                        \"emotion\": emotion,\n",
    "                        \"score\": float(score),\n",
    "                        \"source\": \"cornell\"\n",
    "                    }\n",
    "                })\n",
    "            except Exception as e:\n",
    "                # Log occasional errors\n",
    "                if len(enhanced) % 1000 == 0:\n",
    "                    print(f\"‚ö†Ô∏è Skipped dialogue due to error: {str(e)[:50]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Atomic write with verification\n",
    "        temp_path = f\"{output_path}.tmp\"\n",
    "        with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(enhanced, f, indent=4)\n",
    "\n",
    "        # Verify temp file was created\n",
    "        if not Path(temp_path).exists():\n",
    "            raise Exception(\"Temporary file not created\")\n",
    "\n",
    "        # Atomic move\n",
    "        os.replace(temp_path, output_path)\n",
    "\n",
    "        # Final verification\n",
    "        if not Path(output_path).exists():\n",
    "            raise Exception(\"Final output file missing\")\n",
    "            \n",
    "        print(f\"\\n‚úÖ Successfully saved {len(enhanced)} dialogues to:\")\n",
    "        display(FileLink(output_path))\n",
    "            \n",
    "        return enhanced\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error: {str(e)}\")\n",
    "        if 'temp_path' in locals() and Path(temp_path).exists():\n",
    "            os.remove(temp_path)\n",
    "        return []\n",
    "\n",
    "# ======================\n",
    "# 4. EXECUTION & VERIFICATION\n",
    "# ======================\n",
    "# Run the processing\n",
    "enhanced_data = enhance_with_emotion(\n",
    "    input_path=CLEANED_INPUT,\n",
    "    output_path=ENHANCED_OUTPUT\n",
    ")\n",
    "\n",
    "# Final verification\n",
    "if enhanced_data:\n",
    "    print(\"\\nüîç FINAL VERIFICATION\")\n",
    "    print(f\"Total dialogues processed: {len(enhanced_data)}\")\n",
    "    print(\"Sample metadata:\")\n",
    "    print(enhanced_data[0]['metadata'])\n",
    "    \n",
    "    # Force Kaggle to refresh\n",
    "    !ls -lh \"/kaggle/working/data/\"\n",
    "else:\n",
    "    print(\"\\nüö® Processing failed - no output generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T09:01:58.491407Z",
     "iopub.status.busy": "2025-08-08T09:01:58.490817Z",
     "iopub.status.idle": "2025-08-08T09:01:58.495579Z",
     "shell.execute_reply": "2025-08-08T09:01:58.494800Z",
     "shell.execute_reply.started": "2025-08-08T09:01:58.491382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# ======================\n",
    "# 1. PATH CONFIGURATION\n",
    "# ======================\n",
    "# --- Input Paths ---\n",
    "# This path is based on your emotion tagging script's output\n",
    "CORNELL_EMOTION_INPUT = \"/kaggle/working/data/cleaned_cornell.txt\"\n",
    "# This path is where your preprocessed screenplays should be\n",
    "SCREENPLAY_ROOT_DIR = \"/kaggle/working/data/preprocessed\"\n",
    "\n",
    "# --- Output Path ---\n",
    "FINAL_DATA_OUTPUT = \"/kaggle/working/data/final_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T09:02:12.988904Z",
     "iopub.status.busy": "2025-08-08T09:02:12.988446Z",
     "iopub.status.idle": "2025-08-08T09:02:12.992439Z",
     "shell.execute_reply": "2025-08-08T09:02:12.991681Z",
     "shell.execute_reply.started": "2025-08-08T09:02:12.988880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. SPECIAL TOKENS & METADATA\n",
    "# ======================\n",
    "SPECIAL_TOKENS = {\n",
    "    \"cornell\": \"<CORNELL_DIALOGUE>\",\n",
    "    \"screenplay\": \"<SCREENPLAY>\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T09:27:56.623912Z",
     "iopub.status.busy": "2025-08-08T09:27:56.623085Z",
     "iopub.status.idle": "2025-08-08T09:27:56.635471Z",
     "shell.execute_reply": "2025-08-08T09:27:56.634535Z",
     "shell.execute_reply.started": "2025-08-08T09:27:56.623875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 3. HELPER FUNCTIONS\n",
    "# ======================\n",
    "def load_and_tag_cornell(input_path):\n",
    "    \"\"\"\n",
    "    Loads the emotion-tagged Cornell data and adds the special token\n",
    "    and 'type' metadata.\n",
    "    \"\"\"\n",
    "    if not Path(input_path).exists():\n",
    "        print(f\"‚ùå Cornell file not found: {input_path}\")\n",
    "        return []\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # We're updating the existing data structure\n",
    "    for item in tqdm(data, desc=\"Tagging Cornell data\"):\n",
    "        item[\"text\"] = f\"{SPECIAL_TOKENS['cornell']} {item['text']}\"\n",
    "        item[\"metadata\"][\"type\"] = \"cornell\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_and_tag_screenplays(root_dir):\n",
    "    \"\"\"\n",
    "    Loads all preprocessed screenplay files, combines them,\n",
    "    and adds the special token and type metadata.\n",
    "    \"\"\"\n",
    "    all_screenplays = []\n",
    "    \n",
    "    # The list of screenplay sources to process\n",
    "    sources = [\"imsdb\", \"springfield\", \"screenplaydb\"]\n",
    "    for source in sources:\n",
    "        source_path = Path(root_dir) / source\n",
    "        if not source_path.exists():\n",
    "            print(f\"‚ö†Ô∏è Directory not found for {source}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        script_files = list(source_path.glob(\"*_clean.txt\"))\n",
    "        for script_file in tqdm(script_files, desc=f\"Tagging {source} scripts\"):\n",
    "            try:\n",
    "                with open(script_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read().strip() # .strip() removes leading/trailing whitespace\n",
    "                \n",
    "                # IMPORTANT FIX: Check if content is not empty before processing\n",
    "                if not content:\n",
    "                    print(f\"‚ö†Ô∏è Skipping empty file: {script_file.name}\")\n",
    "                    continue\n",
    "                \n",
    "                tagged_text = f\"{SPECIAL_TOKENS['screenplay']} {content}\"\n",
    "                \n",
    "                all_screenplays.append({\n",
    "                    \"text\": tagged_text,\n",
    "                    \"metadata\": {\n",
    "                        \"type\": \"screenplay\",\n",
    "                        \"source\": source,\n",
    "                        \"filename\": script_file.name\n",
    "                    }\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {script_file.name}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return all_screenplays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T09:27:59.863107Z",
     "iopub.status.busy": "2025-08-08T09:27:59.862398Z",
     "iopub.status.idle": "2025-08-08T09:28:04.690203Z",
     "shell.execute_reply": "2025-08-08T09:28:04.689321Z",
     "shell.execute_reply.started": "2025-08-08T09:27:59.863083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting dataset combination and tagging...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5517e61d0ed40918d15c7899be94c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging Cornell data:   0%|          | 0/304403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe5421e48d647a0bd07cd03e8dba1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging imsdb scripts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping empty file: The-Truman-Show_clean.txt\n",
      "‚ö†Ô∏è Skipping empty file: The-Prestige_clean.txt\n",
      "‚ö†Ô∏è Skipping empty file: The-Matrix_clean.txt\n",
      "‚ö†Ô∏è Skipping empty file: The-Dark-Knight_clean.txt\n",
      "‚ö†Ô∏è Skipping empty file: The-Godfather_clean.txt\n",
      "‚ö†Ô∏è Skipping empty file: The-Shawshank-Redemption_clean.txt\n",
      "‚ö†Ô∏è Skipping empty file: The-Social-Network_clean.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb31d0de4634396a7c55325eeb9c959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging springfield scripts:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8482d21b3e64905a42577aa07a975b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging screenplaydb scripts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Successfully combined 304403 Cornell samples and 25 screenplay samples.\n",
      "Total samples in final dataset: 304428\n",
      "\n",
      "‚úÖ Final dataset saved to: /kaggle/working/data/final_dataset.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================\n",
    "# 4. MAIN EXECUTION\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting dataset combination and tagging...\")\n",
    "\n",
    "    cornell_data = load_and_tag_cornell(CORNELL_EMOTION_INPUT)\n",
    "    screenplay_data = load_and_tag_screenplays(SCREENPLAY_ROOT_DIR)\n",
    "    \n",
    "    final_dataset = cornell_data + screenplay_data\n",
    "    \n",
    "    import random\n",
    "    random.shuffle(final_dataset)\n",
    "    \n",
    "    print(f\"\\nüéâ Successfully combined {len(cornell_data)} Cornell samples and {len(screenplay_data)} screenplay samples.\")\n",
    "    print(f\"Total samples in final dataset: {len(final_dataset)}\")\n",
    "    \n",
    "    try:\n",
    "        with open(FINAL_DATA_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_dataset, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Final dataset saved to: {FINAL_DATA_OUTPUT}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error saving final dataset: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T09:40:46.890406Z",
     "iopub.status.busy": "2025-08-08T09:40:46.890032Z",
     "iopub.status.idle": "2025-08-08T12:35:55.711401Z",
     "shell.execute_reply": "2025-08-08T12:35:55.710587Z",
     "shell.execute_reply.started": "2025-08-08T09:40:46.890384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GPT-2 fine-tuning process...\n",
      "‚úÖ Loaded dataset with 304428 samples.\n",
      "‚úÖ Loading base GPT-2 tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc005ddb4434d49959da29b7d91d3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/304428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading base GPT-2 model.\n",
      "\n",
      "‚è≥ Fine-tuning starting...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51375' max='51375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51375/51375 2:54:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.621600</td>\n",
       "      <td>3.280177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.543200</td>\n",
       "      <td>3.233025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.491000</td>\n",
       "      <td>3.212726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Fine-tuning complete! Model saved to /kaggle/working/deepscript-model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# ======================\n",
    "# 1. PATH CONFIGURATION\n",
    "# ======================\n",
    "FINAL_DATA_INPUT = \"/kaggle/working/data/final_dataset.json\"\n",
    "MODEL_OUTPUT_DIR = \"/kaggle/working/deepscript-model\"\n",
    "\n",
    "# --- NEW: Define the path to your checkpoint ---\n",
    "# You need to manually update this with the name of your latest checkpoint folder,\n",
    "# e.g., \"checkpoint-1234\"\n",
    "RESUME_CHECKPOINT_PATH = \"/kaggle/working/deepscript-model/checkpoint-1234\"\n",
    "\n",
    "# ======================\n",
    "# 2. LOAD DATASET (Same as before)\n",
    "# ======================\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads the final dataset and formats it for training.\"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Final dataset file not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed_data = [{\"text\": item[\"text\"]} for item in data]\n",
    "    dataset = Dataset.from_list(processed_data)\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# ======================\n",
    "# 3. TOKENIZER SETUP (Same as before, but loads from checkpoint if it exists)\n",
    "# ======================\n",
    "def setup_tokenizer(special_tokens, checkpoint_path=None):\n",
    "    \"\"\"Loads GPT-2 tokenizer and adds new special tokens, from a checkpoint if specified.\"\"\"\n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        print(f\"‚úÖ Loading tokenizer from checkpoint: {checkpoint_path}\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "    else:\n",
    "        print(\"‚úÖ Loading base GPT-2 tokenizer.\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': list(special_tokens.values())})\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# ======================\n",
    "# 4. FINE-TUNING EXECUTION\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting GPT-2 fine-tuning process...\")\n",
    "    \n",
    "    SPECIAL_TOKENS = {\n",
    "        \"cornell\": \"<CORNELL_DIALOGUE>\",\n",
    "        \"screenplay\": \"<SCREENPLAY>\"\n",
    "    }\n",
    "\n",
    "    # Load and preprocess the dataset\n",
    "    try:\n",
    "        dataset = load_and_preprocess_data(FINAL_DATA_INPUT)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "        \n",
    "    print(f\"‚úÖ Loaded dataset with {len(dataset)} samples.\")\n",
    "    \n",
    "    # Setup the tokenizer. It will load from the checkpoint if the path is valid.\n",
    "    tokenizer = setup_tokenizer(SPECIAL_TOKENS, RESUME_CHECKPOINT_PATH)\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "        \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Split the dataset into training and validation sets\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = train_test_split['train']\n",
    "    eval_dataset = train_test_split['test']\n",
    "    \n",
    "    # Load the model. It will load from the checkpoint if the path is valid.\n",
    "    if Path(RESUME_CHECKPOINT_PATH).exists():\n",
    "        print(f\"‚úÖ Loading model from checkpoint: {RESUME_CHECKPOINT_PATH}\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(RESUME_CHECKPOINT_PATH)\n",
    "    else:\n",
    "        print(\"‚úÖ Loading base GPT-2 model.\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODEL_OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{MODEL_OUTPUT_DIR}/logs\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Define data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Start fine-tuning. The trainer will automatically resume if the checkpoint path is provided.\n",
    "    print(\"\\n‚è≥ Fine-tuning starting...\")\n",
    "    trainer.train(resume_from_checkpoint=RESUME_CHECKPOINT_PATH if Path(RESUME_CHECKPOINT_PATH).exists() else None)\n",
    "    \n",
    "    # Save the final model and tokenizer\n",
    "    trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "    print(f\"\\n‚úÖ Fine-tuning complete! Model saved to {MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:39:30.093202Z",
     "iopub.status.busy": "2025-08-08T12:39:30.092702Z",
     "iopub.status.idle": "2025-08-08T12:42:18.676389Z",
     "shell.execute_reply": "2025-08-08T12:42:18.675598Z",
     "shell.execute_reply.started": "2025-08-08T12:39:30.093179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Zipping model and tokenizer to /kaggle/working/deepscript-model.zip...\n",
      "‚úÖ Successfully created /kaggle/working/deepscript-model.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# ======================\n",
    "# ZIP THE FINAL MODEL\n",
    "# ======================\n",
    "MODEL_OUTPUT_DIR = \"/kaggle/working/deepscript-model\"\n",
    "ZIP_FILE_PATH = \"/kaggle/working/deepscript-model.zip\"\n",
    "\n",
    "def zip_directory(path, zip_name):\n",
    "    \"\"\"\n",
    "    Zips a directory and all its contents.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to the directory to zip.\n",
    "        zip_name (str): The name of the output zip file.\n",
    "    \"\"\"\n",
    "    if not Path(path).exists():\n",
    "        print(f\"‚ùå Model directory not found at: {path}. Skipping zip creation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚è≥ Zipping model and tokenizer to {zip_name}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Add file to the zip, preserving directory structure\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, path))\n",
    "        print(f\"‚úÖ Successfully created {zip_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating zip file: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_directory(MODEL_OUTPUT_DIR, ZIP_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:49:18.792081Z",
     "iopub.status.busy": "2025-08-08T12:49:18.791508Z",
     "iopub.status.idle": "2025-08-08T12:49:19.115330Z",
     "shell.execute_reply": "2025-08-08T12:49:19.114570Z",
     "shell.execute_reply.started": "2025-08-08T12:49:18.792061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading fine-tuned model and tokenizer...\n",
      "‚úÖ Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# ======================\n",
    "# 1. PATH CONFIGURATION\n",
    "# ======================\n",
    "MODEL_OUTPUT_DIR = \"/kaggle/working/deepscript-model\"\n",
    "\n",
    "# ======================\n",
    "# 2. LOAD MODEL AND TOKENIZER\n",
    "# ======================\n",
    "print(\"‚è≥ Loading fine-tuned model and tokenizer...\")\n",
    "try:\n",
    "    finetuned_tokenizer = GPT2Tokenizer.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "    finetuned_model = GPT2LMHeadModel.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "    finetuned_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"‚úÖ Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "    print(\"Please ensure your fine-tuning script has run and saved the model to the correct directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:49:49.694266Z",
     "iopub.status.busy": "2025-08-08T12:49:49.693984Z",
     "iopub.status.idle": "2025-08-08T12:49:49.699545Z",
     "shell.execute_reply": "2025-08-08T12:49:49.698836Z",
     "shell.execute_reply.started": "2025-08-08T12:49:49.694245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 3. GENERATE TEXT\n",
    "# ======================\n",
    "def generate_text_with_prompt(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Generates text from a given prompt using the fine-tuned model.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Generating text for prompt: '{prompt}'\")\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # These parameters have been adjusted for more coherent output\n",
    "    output_tokens = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.8,      # Lowered top_p from 0.95 to 0.8\n",
    "        temperature=0.6, # Lowered temperature from 0.7 to 0.6\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=False)\n",
    "    print(\"\\n--- Generated Output ---\")\n",
    "    print(generated_text)\n",
    "    print(\"------------------------\")\n",
    "    return generated_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:54:38.423044Z",
     "iopub.status.busy": "2025-08-08T12:54:38.422775Z",
     "iopub.status.idle": "2025-08-08T12:54:40.139070Z",
     "shell.execute_reply": "2025-08-08T12:54:40.138331Z",
     "shell.execute_reply.started": "2025-08-08T12:54:38.423022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Generating text for prompt: '<SCREENPLAY> INT. ABANDONED WAREHOUSE - NIGHT'\n",
      "\n",
      "--- Generated Output ---\n",
      "<SCREENPLAY>  INT. ABANDONED WAREHOUSE - NIGHT. The door opens and a young man is standing in the doorway. He looks around, then turns back to the door. A young woman is walking down the hall. She is wearing a black dress. Her hair is in a bun. Behind her is a man. His face is blank. It is very dark. At the end of the hallway is another young MAN. They both look at him. Then they look back at each\n",
      "------------------------\n",
      "\n",
      "üìù Generating text for prompt: '<CORNELL_DIALOGUE> MARK: Hey man, what's up? \n",
      "JOHN: '\n",
      "\n",
      "--- Generated Output ---\n",
      "<CORNELL_DIALOGUE>  MARK: Hey man, what's up? \n",
      "JOHN:  I'm not getting a job.  And I don't know if I can afford it. I gotta get out of here. There's no way I could afford to. So I'll just have to get some money and get back to my old life. How's that? I mean, I've got a lot of work to do. What's the matter with you? You don¬ít have a car.\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 4. RUN GENERATION EXAMPLES\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Generate a screenplay scene\n",
    "    screenplay_prompt = \"<SCREENPLAY> INT. ABANDONED WAREHOUSE - NIGHT\"\n",
    "    generate_text_with_prompt(finetuned_model, finetuned_tokenizer, screenplay_prompt)\n",
    "    \n",
    "    # Example 2: Generate a dialogue with a new prompt\n",
    "     # Example 2: Generate a dialogue between two guys\n",
    "    dialogue_prompt = \"<CORNELL_DIALOGUE> MARK: Hey man, what's up? \\nJOHN: \"\n",
    "    generate_text_with_prompt(finetuned_model, finetuned_tokenizer, dialogue_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# This script fine-tunes a GPT-2 model on a dialogue dataset that has\n",
    "# been pre-processed with emotion tags.\n",
    "# The goal is to train the model to generate conversations that\n",
    "# adhere to a specific format: [EMOTION] SPEAKER: TEXT.\n",
    "# ====================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ======================\n",
    "# 1. AUTHENTICATION & PATHS\n",
    "# ======================\n",
    "# Your Hugging Face token has been added here.\n",
    "\n",
    "\n",
    "# Path to your cleaned and emotion-tagged dataset\n",
    "CLEANED_DATASET_PATH = \"/kaggle/working/data/cleaned_cornell.txt\"\n",
    "\n",
    "# Path where the fine-tuned model will be saved\n",
    "OUTPUT_DIR = \"/kaggle/working/emotion-model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T13:42:44.667682Z",
     "iopub.status.busy": "2025-08-08T13:42:44.667045Z",
     "iopub.status.idle": "2025-08-08T13:42:44.672669Z",
     "shell.execute_reply": "2025-08-08T13:42:44.672077Z",
     "shell.execute_reply.started": "2025-08-08T13:42:44.667662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. DATA PREPARATION & LOADING\n",
    "# ======================\n",
    "def load_and_format_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads the JSON data and formats each entry into a single string\n",
    "    for fine-tuning the language model.\n",
    "    \"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"‚ùå Error: Dataset file not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(\"‚è≥ Loading and formatting dataset...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_texts = []\n",
    "    for item in data:\n",
    "        character = item['metadata']['character']\n",
    "        emotion = item['metadata']['emotion']\n",
    "        text = item['text'].split(\": \", 1)[-1]  # Get the text after the character name\n",
    "        # Create a single string in the format the model should learn\n",
    "        formatted_text = f\"[{emotion.upper()}] {character}: {text}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(formatted_texts)} dialogues.\")\n",
    "    return formatted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T13:42:45.027021Z",
     "iopub.status.busy": "2025-08-08T13:42:45.026798Z",
     "iopub.status.idle": "2025-08-08T13:42:45.031227Z",
     "shell.execute_reply": "2025-08-08T13:42:45.030697Z",
     "shell.execute_reply.started": "2025-08-08T13:42:45.027003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 3. MODEL AND TOKENIZER SETUP\n",
    "# ======================\n",
    "def setup_model_and_tokenizer():\n",
    "    \"\"\"\n",
    "    Initializes the GPT-2 tokenizer and model for fine-tuning.\n",
    "    \"\"\"\n",
    "    print(\"‚è≥ Setting up model and tokenizer...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Add a new pad token and resize the model's token embeddings\n",
    "    # This is crucial for handling variable-length sequences\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    print(\"‚úÖ Model and tokenizer set up successfully.\")\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T13:46:56.869420Z",
     "iopub.status.busy": "2025-08-08T13:46:56.869103Z",
     "iopub.status.idle": "2025-08-08T13:46:56.877426Z",
     "shell.execute_reply": "2025-08-08T13:46:56.876655Z",
     "shell.execute_reply.started": "2025-08-08T13:46:56.869396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 4. FINE-TUNING FUNCTION\n",
    "# ======================\n",
    "def fine_tune_model():\n",
    "    \"\"\"\n",
    "    Loads the data, sets up the model, and runs the fine-tuning process.\n",
    "    \"\"\"\n",
    "    formatted_data = load_and_format_dataset(CLEANED_DATASET_PATH)\n",
    "    if not formatted_data:\n",
    "        return\n",
    "\n",
    "    tokenizer, model = setup_model_and_tokenizer()\n",
    "\n",
    "    # Convert our formatted data into a Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict({'text': formatted_data})\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Set the `labels` for language modeling\n",
    "    tokenized_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    # Set labels as input_ids for Causal Language Modeling\n",
    "    def set_labels(examples):\n",
    "        examples[\"labels\"] = examples[\"input_ids\"].clone()\n",
    "        return examples\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.map(set_labels, batched=True)\n",
    "\n",
    "    # --- NEW: SPLIT THE DATASET FOR TRAINING AND VALIDATION ---\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split_dataset['train']\n",
    "    eval_dataset = split_dataset['test']\n",
    "    print(f\"‚úÖ Dataset split: {len(train_dataset)} samples for training, {len(eval_dataset)} for validation.\")\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,  # Adjusted to 3 epochs\n",
    "        per_device_train_batch_size=8,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        evaluation_strategy=\"epoch\",  # NEW: Evaluate at the end of each epoch\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer with both train and evaluation datasets\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset, # NEW: Pass the validation dataset here\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Starting fine-tuning...\")\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T13:46:58.456405Z",
     "iopub.status.busy": "2025-08-08T13:46:58.455692Z",
     "iopub.status.idle": "2025-08-08T16:42:50.223591Z",
     "shell.execute_reply": "2025-08-08T16:42:50.222326Z",
     "shell.execute_reply.started": "2025-08-08T13:46:58.456379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51375' max='51375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51375/51375 2:54:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.535800</td>\n",
       "      <td>3.254994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.478600</td>\n",
       "      <td>3.225784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.438100</td>\n",
       "      <td>3.215687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuning complete.\n",
      "\n",
      "‚ú® Final model saved to /kaggle/working/emotion-model\n",
      "‚è≥ Loading and formatting dataset...\n",
      "‚úÖ Loaded 304403 dialogues.\n",
      "‚è≥ Setting up model and tokenizer...\n",
      "‚úÖ Model and tokenizer set up successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded5851ba24847f39a0f9e9271c717b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/304403 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e724f0d3cc4344ed8361f85ee3b14679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/304403 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset split: 273962 samples for training, 30441 for validation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3080665186.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ======================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_36/3485877411.py\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "    # Fine-tune the model\n",
    "trainer.train()\n",
    "print(\"‚úÖ Fine-tuning complete.\")\n",
    "    \n",
    "    # Save the final model and tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(f\"\\n‚ú® Final model saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "# ======================\n",
    "# 5. EXECUTION\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    fine_tune_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T18:49:25.378216Z",
     "iopub.status.busy": "2025-08-08T18:49:25.377656Z",
     "iopub.status.idle": "2025-08-08T18:49:25.528414Z",
     "shell.execute_reply": "2025-08-08T18:49:25.527767Z",
     "shell.execute_reply.started": "2025-08-08T18:49:25.378194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCENE ===\n",
      "Three boys went shopping at the mall\n",
      "\n",
      "=== CONVERSATION ===\n",
      "[neutral] Bishwa: What do you think of these choosing colors?\n",
      "[neutral] Aadarsha: This section has good options.\n",
      "Bishwa: [interrupting]\n",
      "[confused] Bishwa: What do you think of these looking at shirts?\n",
      "[neutral] Aaditya: What do you think of these trying on jeans?\n",
      "[confused] Bishwa: This section has good options.\n",
      "[happy] Aadarsha: I really like this jacket.\n",
      "Aaditya: [interrupting]\n",
      "[confused] Aaditya: Should we check out the new arrivals?\n",
      "[realization] Bishwa: Should we check out the new arrivals?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"/kaggle/working/emotion-model\"\n",
    "MAX_HISTORY = 3\n",
    "INTERRUPTION_CHANCE = 0.15\n",
    "\n",
    "# Enhanced emotion system with better transitions\n",
    "EMOTIONS = {\n",
    "    'neutral': ['happy', 'curious', 'confused'],\n",
    "    'happy': ['excited', 'neutral', 'amused'],\n",
    "    'angry': ['annoyed', 'frustrated', 'neutral'],\n",
    "    'confused': ['realization', 'frustration', 'neutral'],\n",
    "    'excited': ['happy', 'enthusiastic', 'neutral'],\n",
    "    'annoyed': ['angry', 'frustrated', 'neutral'],\n",
    "    'realization': ['happy', 'neutral', 'surprised']\n",
    "}\n",
    "\n",
    "class ShoppingDialogue:\n",
    "    def __init__(self):\n",
    "        self.speakers = [\"Aadarsha\", \"Aaditya\", \"Bishwa\"]\n",
    "        self.scene = \"Three boys went shopping at the mall\"\n",
    "        self.history = []\n",
    "        self.current_emotions = {speaker: 'neutral' for speaker in self.speakers}\n",
    "        \n",
    "    def run(self):\n",
    "        print(f\"\\n=== SCENE ===\\n{self.scene}\\n\")\n",
    "        print(\"=== CONVERSATION ===\")\n",
    "        \n",
    "        current_speaker = random.choice(self.speakers)\n",
    "        for _ in range(8):  # 8 turns as requested\n",
    "            # Generate appropriate dialogue\n",
    "            dialogue = self.generate_dialogue(current_speaker)\n",
    "            self.history.append(dialogue)\n",
    "            print(f\"[{self.current_emotions[current_speaker]}] {current_speaker}: {dialogue}\")\n",
    "            \n",
    "            # Update emotion naturally\n",
    "            self.update_emotion(current_speaker, dialogue)\n",
    "            \n",
    "            # Handle interruptions\n",
    "            if random.random() < INTERRUPTION_CHANCE:\n",
    "                interrupter = random.choice([s for s in self.speakers if s != current_speaker])\n",
    "                print(f\"{interrupter}: [interrupting]\")\n",
    "                current_speaker = interrupter\n",
    "            else:\n",
    "                current_speaker = random.choice([s for s in self.speakers if s != current_speaker])\n",
    "\n",
    "    def generate_dialogue(self, speaker):\n",
    "        \"\"\"Generate context-appropriate shopping dialogue\"\"\"\n",
    "        topics = [\n",
    "            \"looking at shirts\", \n",
    "            \"checking out shoes\",\n",
    "            \"trying on jeans\",\n",
    "            \"comparing prices\",\n",
    "            \"deciding what to buy\",\n",
    "            \"looking for deals\",\n",
    "            \"discussing brands\",\n",
    "            \"choosing colors\"\n",
    "        ]\n",
    "        templates = [\n",
    "            f\"What do you think of these {random.choice(topics)}?\",\n",
    "            f\"I really like this {random.choice(['shirt', 'jacket', 'pair of shoes'])}.\",\n",
    "            f\"Should we check out the {random.choice(['sale section', 'new arrivals', 'accessories'])}?\",\n",
    "            f\"This {random.choice(['store', 'brand', 'section'])} has good options.\",\n",
    "            f\"Let's go look at {random.choice(['the other floor', 'another store', 'the food court'])} after this.\"\n",
    "        ]\n",
    "        \n",
    "        # Ensure dialogue stays on shopping topic\n",
    "        return random.choice(templates)\n",
    "\n",
    "    def update_emotion(self, speaker, dialogue):\n",
    "        \"\"\"Update emotion based on dialogue content\"\"\"\n",
    "        current = self.current_emotions[speaker]\n",
    "        \n",
    "        # Simple emotion progression\n",
    "        if '?' in dialogue:\n",
    "            self.current_emotions[speaker] = 'confused' if random.random() < 0.5 else 'curious'\n",
    "        elif '!' in dialogue:\n",
    "            self.current_emotions[speaker] = 'excited'\n",
    "        elif any(word in dialogue.lower() for word in ['like', 'love', 'great']):\n",
    "            self.current_emotions[speaker] = 'happy'\n",
    "        elif any(word in dialogue.lower() for word in ['hate', 'annoying', 'bad']):\n",
    "            self.current_emotions[speaker] = 'angry'\n",
    "        else:\n",
    "            # Normal transition\n",
    "            self.current_emotions[speaker] = random.choice(EMOTIONS[current])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        print(f\"Error: Model not found at {MODEL_DIR}\")\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)\n",
    "        model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "        dialogue = ShoppingDialogue()\n",
    "        dialogue.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T18:42:32.185326Z",
     "iopub.status.busy": "2025-08-08T18:42:32.184817Z",
     "iopub.status.idle": "2025-08-08T18:43:25.378229Z",
     "shell.execute_reply": "2025-08-08T18:43:25.377562Z",
     "shell.execute_reply.started": "2025-08-08T18:42:32.185297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "=== SPEAKER SETUP ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of speakers (2-6):  4\n",
      "Enter name for speaker 1:  aadarsha\n",
      "Enter name for speaker 2:  aaditya\n",
      "Enter name for speaker 3:  bishwa\n",
      "Enter name for speaker 4:  jagdish\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCENE SETUP ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter scene description:  boys went for shopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONVERSATION START ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of dialogue turns (3-20):  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[confusion] aaditya: You're not a man, are you? And what is this man's name? He's a real man. What's his name, Mr. C.? My name is Mr Rothstein. Mr C is the guy I.\n",
      "[disapproval] aadarsha: That's not neutral. It's neutral, is it not? It is neutral! Now, why are we here? Because we're here, because we are here! It was neutral ! It can't be neutral now. Now listen, I know.\n",
      "[confusion] bishwa: I'll get it back to you. Your name. Your friend. You know what you're doing. Aadasha. But I'm not interested in it. I want to talk to your friends. They're all here. We're.\n",
      "[disapproval] aaditya: It'll be a good day. There's nothing here but confusion.     You were the one who told me you were here to see the movie. You can be here tonight. If you.\n",
      "bishwa: [interrupting]\n",
      "[disapproval] bishwa: You were in the theater.  You are now. Because you are not. It's a film. This is not a movie, it's not even a picture. A film, by the way, a television show.\n",
      "aaditya: [interrupting]\n",
      "[annoyance] aaditya: interrupted  interrupted     interrupt       to the point where the subject is aware of the existence of an intentionality.\n",
      "\n",
      "=== FINAL CONVERSATION ===\n",
      "[neutral] aaditya: You're not a man, are you? And what is this man's name? He's a real man. What's his name, Mr. C.? My name is Mr Rothstein. Mr C is the guy I.\n",
      "[neutral] aadarsha: That's not neutral. It's neutral, is it not? It is neutral! Now, why are we here? Because we're here, because we are here! It was neutral ! It can't be neutral now. Now listen, I know.\n",
      "[neutral] bishwa: I'll get it back to you. Your name. Your friend. You know what you're doing. Aadasha. But I'm not interested in it. I want to talk to your friends. They're all here. We're.\n",
      "[confusion] aaditya: It'll be a good day. There's nothing here but confusion.     You were the one who told me you were here to see the movie. You can be here tonight. If you.\n",
      "bishwa: [interrupting]\n",
      "[confusion] bishwa: You were in the theater.  You are now. Because you are not. It's a film. This is not a movie, it's not even a picture. A film, by the way, a television show.\n",
      "aaditya: [interrupting]\n",
      "[disapproval] aaditya: interrupted  interrupted     interrupt       to the point where the subject is aware of the existence of an intentionality.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"/kaggle/working/emotion-model\"\n",
    "MAX_HISTORY = 3\n",
    "INTERRUPTION_CHANCE = 0.15\n",
    "\n",
    "# Complete 28 emotions with detailed transitions\n",
    "EMOTIONS = {\n",
    "    'admiration': {\n",
    "        'transitions': ['approval', 'gratitude', 'neutral'],\n",
    "        'trigger_words': ['impressed', 'wow', 'amazing']\n",
    "    },\n",
    "    'amusement': {\n",
    "        'transitions': ['joy', 'excitement', 'neutral'],\n",
    "        'trigger_words': ['laugh', 'funny', 'hilarious']\n",
    "    },\n",
    "    'anger': {\n",
    "        'transitions': ['annoyance', 'frustration', 'rage'],\n",
    "        'trigger_words': ['angry', 'mad', 'furious']\n",
    "    },\n",
    "    'annoyance': {\n",
    "        'transitions': ['irritation', 'frustration', 'neutral'],\n",
    "        'trigger_words': ['annoying', 'bother', 'ugh']\n",
    "    },\n",
    "    'approval': {\n",
    "        'transitions': ['admiration', 'gratitude', 'neutral'],\n",
    "        'trigger_words': ['agree', 'support', 'yes']\n",
    "    },\n",
    "    'caring': {\n",
    "        'transitions': ['love', 'gratitude', 'neutral'],\n",
    "        'trigger_words': ['care', 'concern', 'worry']\n",
    "    },\n",
    "    'confusion': {\n",
    "        'transitions': ['curiosity', 'realization', 'neutral'],\n",
    "        'trigger_words': ['confused', 'dont understand', 'what']\n",
    "    },\n",
    "    'curiosity': {\n",
    "        'transitions': ['confusion', 'excitement', 'neutral'],\n",
    "        'trigger_words': ['wonder', 'ask', 'question']\n",
    "    },\n",
    "    'desire': {\n",
    "        'transitions': ['excitement', 'anticipation', 'neutral'],\n",
    "        'trigger_words': ['want', 'wish', 'desire']\n",
    "    },\n",
    "    'disappointment': {\n",
    "        'transitions': ['sadness', 'regret', 'neutral'],\n",
    "        'trigger_words': ['let down', 'disappointed', 'unhappy']\n",
    "    },\n",
    "    'disapproval': {\n",
    "        'transitions': ['anger', 'annoyance', 'neutral'],\n",
    "        'trigger_words': ['disagree', 'against', 'no']\n",
    "    },\n",
    "    'disgust': {\n",
    "        'transitions': ['contempt', 'anger', 'neutral'],\n",
    "        'trigger_words': ['gross', 'disgusting', 'ew']\n",
    "    },\n",
    "    'embarrassment': {\n",
    "        'transitions': ['shame', 'regret', 'neutral'],\n",
    "        'trigger_words': ['embarrassed', 'awkward', 'cringe']\n",
    "    },\n",
    "    'excitement': {\n",
    "        'transitions': ['joy', 'anticipation', 'neutral'],\n",
    "        'trigger_words': ['excited', 'thrilled', 'can\\'t wait']\n",
    "    },\n",
    "    'fear': {\n",
    "        'transitions': ['anxiety', 'nervousness', 'neutral'],\n",
    "        'trigger_words': ['scared', 'afraid', 'fear']\n",
    "    },\n",
    "    'gratitude': {\n",
    "        'transitions': ['approval', 'admiration', 'neutral'],\n",
    "        'trigger_words': ['thank', 'appreciate', 'grateful']\n",
    "    },\n",
    "    'grief': {\n",
    "        'transitions': ['sadness', 'despair', 'neutral'],\n",
    "        'trigger_words': ['loss', 'mourn', 'heartbroken']\n",
    "    },\n",
    "    'joy': {\n",
    "        'transitions': ['amusement', 'excitement', 'neutral'],\n",
    "        'trigger_words': ['happy', 'joyful', 'delighted']\n",
    "    },\n",
    "    'love': {\n",
    "        'transitions': ['caring', 'admiration', 'neutral'],\n",
    "        'trigger_words': ['love', 'adore', 'cherish']\n",
    "    },\n",
    "    'nervousness': {\n",
    "        'transitions': ['fear', 'anxiety', 'neutral'],\n",
    "        'trigger_words': ['nervous', 'anxious', 'worried']\n",
    "    },\n",
    "    'optimism': {\n",
    "        'transitions': ['hope', 'excitement', 'neutral'],\n",
    "        'trigger_words': ['optimistic', 'hopeful', 'positive']\n",
    "    },\n",
    "    'pride': {\n",
    "        'transitions': ['confidence', 'satisfaction', 'neutral'],\n",
    "        'trigger_words': ['proud', 'accomplished', 'achievement']\n",
    "    },\n",
    "    'realization': {\n",
    "        'transitions': ['surprise', 'understanding', 'neutral'],\n",
    "        'trigger_words': ['realize', 'understand', 'oh']\n",
    "    },\n",
    "    'relief': {\n",
    "        'transitions': ['gratitude', 'contentment', 'neutral'],\n",
    "        'trigger_words': ['relieved', 'thankful', 'phew']\n",
    "    },\n",
    "    'remorse': {\n",
    "        'transitions': ['regret', 'guilt', 'neutral'],\n",
    "        'trigger_words': ['sorry', 'apologize', 'regret']\n",
    "    },\n",
    "    'sadness': {\n",
    "        'transitions': ['grief', 'loneliness', 'neutral'],\n",
    "        'trigger_words': ['sad', 'upset', 'depressed']\n",
    "    },\n",
    "    'surprise': {\n",
    "        'transitions': ['shock', 'amazement', 'neutral'],\n",
    "        'trigger_words': ['surprised', 'wow', 'shocked']\n",
    "    },\n",
    "    'neutral': {\n",
    "        'transitions': ['curiosity', 'interest', 'contentment'],\n",
    "        'trigger_words': []\n",
    "    }\n",
    "}\n",
    "\n",
    "class DeepScriptDialogue:\n",
    "    def __init__(self):\n",
    "        self.speakers = []\n",
    "        self.scene = \"\"\n",
    "        self.history = []\n",
    "        self.states = {}\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the complete dialogue system\"\"\"\n",
    "        self._load_model()\n",
    "        self._get_speakers()\n",
    "        self._get_scene()\n",
    "        self._initialize_states()\n",
    "        self._start_conversation()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the GPT-2 model and tokenizer\"\"\"\n",
    "        if not os.path.exists(MODEL_DIR):\n",
    "            raise FileNotFoundError(f\"Model not found at {MODEL_DIR}\")\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "    def _get_speakers(self):\n",
    "        \"\"\"Get speaker information from user\"\"\"\n",
    "        print(\"\\n=== SPEAKER SETUP ===\")\n",
    "        \n",
    "        # Get number of speakers\n",
    "        while True:\n",
    "            try:\n",
    "                num_speakers = int(input(\"Enter number of speakers (2-6): \"))\n",
    "                if 2 <= num_speakers <= 6:\n",
    "                    break\n",
    "                print(\"Please enter between 2 and 6 speakers\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "\n",
    "        # Get each speaker's name\n",
    "        self.speakers = []\n",
    "        for i in range(1, num_speakers + 1):\n",
    "            while True:\n",
    "                name = input(f\"Enter name for speaker {i}: \").strip()\n",
    "                if name:\n",
    "                    self.speakers.append(name)\n",
    "                    break\n",
    "                print(\"Name cannot be empty\")\n",
    "\n",
    "    def _get_scene(self):\n",
    "        \"\"\"Get scene description from user\"\"\"\n",
    "        print(\"\\n=== SCENE SETUP ===\")\n",
    "        while True:\n",
    "            self.scene = input(\"Enter scene description: \").strip()\n",
    "            if self.scene:\n",
    "                break\n",
    "            print(\"Scene description cannot be empty\")\n",
    "\n",
    "    def _initialize_states(self):\n",
    "        \"\"\"Initialize emotional states for each speaker\"\"\"\n",
    "        self.states = {}\n",
    "        initial_emotion = self._determine_initial_emotion()\n",
    "        \n",
    "        for speaker in self.speakers:\n",
    "            self.states[speaker] = {\n",
    "                'emotion': initial_emotion,\n",
    "                'history': [],\n",
    "                'interruptions': 0\n",
    "            }\n",
    "\n",
    "    def _determine_initial_emotion(self) -> str:\n",
    "        \"\"\"Determine initial emotion based on scene context\"\"\"\n",
    "        scene_lower = self.scene.lower()\n",
    "        \n",
    "        if any(word in scene_lower for word in ['argue', 'fight', 'conflict']):\n",
    "            return 'anger'\n",
    "        elif any(word in scene_lower for word in ['happy', 'celebrate', 'joy']):\n",
    "            return 'joy'\n",
    "        elif any(word in scene_lower for word in ['sad', 'grief', 'loss']):\n",
    "            return 'sadness'\n",
    "        elif any(word in scene_lower for word in ['discuss', 'talk', 'meet']):\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    def _start_conversation(self):\n",
    "        \"\"\"Begin the dialogue generation\"\"\"\n",
    "        print(\"\\n=== CONVERSATION START ===\")\n",
    "        \n",
    "        # Get number of turns\n",
    "        while True:\n",
    "            try:\n",
    "                turns = int(input(\"Enter number of dialogue turns (3-20): \"))\n",
    "                if 3 <= turns <= 20:\n",
    "                    break\n",
    "                print(\"Please enter between 3 and 20 turns\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "\n",
    "        # Start with random speaker\n",
    "        current_speaker = random.choice(self.speakers)\n",
    "        \n",
    "        for _ in range(turns):\n",
    "            # Check for interruption\n",
    "            if random.random() < INTERRUPTION_CHANCE and len(self.speakers) > 1:\n",
    "                interrupter = random.choice([s for s in self.speakers if s != current_speaker])\n",
    "                self._handle_interruption(current_speaker, interrupter)\n",
    "                current_speaker = interrupter\n",
    "                continue\n",
    "            \n",
    "            # Generate dialogue\n",
    "            dialogue = self._generate_dialogue(current_speaker)\n",
    "            self._update_history(current_speaker, dialogue)\n",
    "            \n",
    "            # Update emotion state\n",
    "            self._update_emotion(current_speaker, dialogue)\n",
    "            \n",
    "            # Print output\n",
    "            print(f\"[{self.states[current_speaker]['emotion']}] {current_speaker}: {dialogue}\")\n",
    "            \n",
    "            # Switch speaker\n",
    "            current_speaker = random.choice([s for s in self.speakers if s != current_speaker])\n",
    "\n",
    "        self._print_final_conversation()\n",
    "\n",
    "    def _generate_dialogue(self, speaker: str) -> str:\n",
    "        \"\"\"Generate dialogue for given speaker\"\"\"\n",
    "        prompt = self._build_prompt(speaker)\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=len(inputs[0]) + 50,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return self._clean_text(generated[len(prompt):])\n",
    "\n",
    "    def _build_prompt(self, speaker: str) -> str:\n",
    "        \"\"\"Build the context prompt for generation\"\"\"\n",
    "        prompt_parts = [\n",
    "            f\"Scene: {self.scene}\",\n",
    "            \"Current conversation:\",\n",
    "            *self.history[-MAX_HISTORY:],\n",
    "            f\"Current emotion: {self.states[speaker]['emotion']}\",\n",
    "            f\"{speaker}:\"\n",
    "        ]\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and format generated text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\']', '', text)  # Remove special chars\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Ensure proper sentence ending\n",
    "        if not any(text.endswith(p) for p in ('.', '?', '!', '\"', \"'\")):\n",
    "            text += '.'\n",
    "        \n",
    "        return text[:500]  # Limit length\n",
    "\n",
    "    def _update_history(self, speaker: str, dialogue: str):\n",
    "        \"\"\"Update conversation history\"\"\"\n",
    "        entry = f\"[{self.states[speaker]['emotion']}] {speaker}: {dialogue}\"\n",
    "        self.history.append(entry)\n",
    "        self.states[speaker]['history'].append(entry)\n",
    "\n",
    "    def _update_emotion(self, speaker: str, dialogue: str):\n",
    "        \"\"\"Update speaker's emotional state based on context\"\"\"\n",
    "        current_emotion = self.states[speaker]['emotion']\n",
    "        \n",
    "        # Check for emotion triggers in dialogue\n",
    "        for emotion, data in EMOTIONS.items():\n",
    "            if any(trigger in dialogue.lower() for trigger in data['trigger_words']):\n",
    "                self.states[speaker]['emotion'] = emotion\n",
    "                return\n",
    "        \n",
    "        # If no triggers found, use normal transition\n",
    "        possible_transitions = EMOTIONS[current_emotion]['transitions']\n",
    "        self.states[speaker]['emotion'] = random.choice(possible_transitions)\n",
    "\n",
    "    def _handle_interruption(self, current_speaker: str, interrupter: str):\n",
    "        \"\"\"Handle conversation interruption\"\"\"\n",
    "        interruption_text = f\"{interrupter}: [interrupting]\"\n",
    "        self.history.append(interruption_text)\n",
    "        self.states[interrupter]['interruptions'] += 1\n",
    "        print(interruption_text)\n",
    "\n",
    "    def _print_final_conversation(self):\n",
    "        \"\"\"Print the complete conversation\"\"\"\n",
    "        print(\"\\n=== FINAL CONVERSATION ===\")\n",
    "        for line in self.history:\n",
    "            print(line)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dialogue_system = DeepScriptDialogue()\n",
    "    dialogue_system.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 424454,
     "modelInstanceId": 406541,
     "sourceId": 514245,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
